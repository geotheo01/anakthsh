{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "import-libraries",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Georg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "# Κατέβασμα απαραίτητων δεδομένων από NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "preprocess-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Συνάρτηση προεπεξεργασίας κειμένου\n",
    "def preprocess_text(content):\n",
    "    tokens = word_tokenize(content)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    filtered_tokens = [token for token in lemmatized_tokens if token.lower() not in stop_words]\n",
    "    cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token) for token in filtered_tokens]\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "build-inverted-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιουργία ανεστραμμένου ευρετηρίου\n",
    "def build_inverted_index(data):\n",
    "    inverted_index = defaultdict(list)\n",
    "    for doc_id, document in enumerate(data):\n",
    "        terms = set(document[\"processed_content\"].split())\n",
    "        for term in terms:\n",
    "            inverted_index[term].append(doc_id)\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "tfidf-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Υπολογισμός TF-IDF\n",
    "def calculate_tfidf(query_tokens, all_data, inverted_index):\n",
    "    N = len(all_data)\n",
    "    scores = defaultdict(float)\n",
    "    for term in query_tokens:\n",
    "        doc_ids = inverted_index.get(term, [])\n",
    "        df = len(doc_ids)\n",
    "        if df > 0:\n",
    "            idf = math.log(N / df)\n",
    "            for doc_id in doc_ids:\n",
    "                tf = all_data[doc_id][\"processed_content\"].split().count(term)\n",
    "                scores[doc_id] += tf * idf\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "boolean-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Retrieval\n",
    "def process_query_boolean(query, inverted_index):\n",
    "    query_tokens = word_tokenize(query.lower())\n",
    "    results = set()\n",
    "    current_set = set()\n",
    "    operator = None\n",
    "    for token in query_tokens:\n",
    "        if token == \"and\":\n",
    "            operator = \"AND\"\n",
    "        elif token == \"or\":\n",
    "            operator = \"OR\"\n",
    "        elif token == \"not\":\n",
    "            operator = \"NOT\"\n",
    "        else:\n",
    "            current_set = set(inverted_index.get(token, []))\n",
    "            if operator == \"AND\":\n",
    "                results &= current_set\n",
    "            elif operator == \"OR\":\n",
    "                results |= current_set\n",
    "            elif operator == \"NOT\":\n",
    "                results -= current_set\n",
    "            else:\n",
    "                results = current_set\n",
    "    return list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bm25-retrieval",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okapi BM25\n",
    "def process_query_bm25(query, all_data, inverted_index, k1=1.5, b=0.75):\n",
    "    query_tokens = word_tokenize(preprocess_text(query).lower())\n",
    "    N = len(all_data)\n",
    "    avg_doc_len = sum(len(doc[\"processed_content\"].split()) for doc in all_data) / N\n",
    "    scores = defaultdict(float)\n",
    "    for term in query_tokens:\n",
    "        doc_ids = inverted_index.get(term, [])\n",
    "        df = len(doc_ids)\n",
    "        if df > 0:\n",
    "            idf = math.log((N - df + 0.5) / (df + 0.5) + 1)\n",
    "            for doc_id in doc_ids:\n",
    "                doc = all_data[doc_id]\n",
    "                term_freq = doc[\"processed_content\"].split().count(term)\n",
    "                doc_len = len(doc[\"processed_content\"].split())\n",
    "                numerator = term_freq * (k1 + 1)\n",
    "                denominator = term_freq + k1 * (1 - b + b * (doc_len / avg_doc_len))\n",
    "                scores[doc_id] += idf * (numerator / denominator)\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c0b04017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector Space Model (VSM)\n",
    "def process_query_vsm(query, all_data, inverted_index):\n",
    "    query_tokens = word_tokenize(preprocess_text(query).lower())\n",
    "    N = len(all_data)\n",
    "    scores = defaultdict(float)\n",
    "\n",
    "    # Υπολογισμός IDF για τους όρους του ερωτήματος\n",
    "    idf = {}\n",
    "    for term in query_tokens:\n",
    "        df = len(inverted_index.get(term, []))\n",
    "        idf[term] = math.log(N / (1 + df)) if df > 0 else 0\n",
    "\n",
    "    # Υπολογισμός βαθμολογιών για κάθε έγγραφο\n",
    "    for term in query_tokens:\n",
    "        for doc_id in inverted_index.get(term, []):\n",
    "            tf = all_data[doc_id][\"processed_content\"].split().count(term)\n",
    "            scores[doc_id] += tf * idf[term]\n",
    "\n",
    "    # Κανονικοποίηση βαθμολογιών\n",
    "    for doc_id in scores:\n",
    "        doc_length = len(all_data[doc_id][\"processed_content\"].split())\n",
    "        scores[doc_id] /= doc_length\n",
    "\n",
    "    return sorted(scores.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "data-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Συλλογή δεδομένων από URLs\n",
    "urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Web_scraping\",\n",
    "    \"https://en.wikipedia.org/wiki/Data_mining\",\n",
    "    \"https://en.wikipedia.org/wiki/Natural_language_processing\",\n",
    "    \"https://en.wikipedia.org/wiki/Artificial_intelligence\",\n",
    "    \"https://en.wikipedia.org/wiki/Savory_spinach_pie\",\n",
    "    \"https://en.wikipedia.org/wiki/Chocolate\",\n",
    "    \"https://en.wikipedia.org/wiki/Plumber\",\n",
    "    \"https://en.wikipedia.org/wiki/University_of_West_Attica\",\n",
    "    \"https://en.wikipedia.org/wiki/McDonald%27s\",\n",
    "    \"https://en.wikipedia.org/wiki/Cancer\",\n",
    "    \"https://en.wikipedia.org/wiki/Gastroenteritis\",\n",
    "    \"https://en.wikipedia.org/wiki/Diabetes\"\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "for url in urls:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            title = soup.find(\"h1\", id=\"firstHeading\").text\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            content = \"\\n\".join([p.text for p in paragraphs if p.text.strip()])\n",
    "            processed_content = preprocess_text(content)\n",
    "            all_data.append({\n",
    "                \"title\": title,\n",
    "                \"content\": content,\n",
    "                \"processed_content\": processed_content\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9efdc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Τα δεδομένα αποθηκεύτηκαν στο collected_data.json\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση δεδομένων σε JSON\n",
    "data_file_path = \"collected_data.json\"\n",
    "with open(data_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(all_data, json_file, ensure_ascii=False, indent=4)\n",
    "print(f\"Τα δεδομένα αποθηκεύτηκαν στο {data_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "search-and-display",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Δημιουργία ανεστραμμένου ευρετηρίου\n",
    "inverted_index = build_inverted_index(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91719013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο inverted_index.json\n"
     ]
    }
   ],
   "source": [
    "# Αποθήκευση ανεστραμμένου ευρετηρίου σε JSON\n",
    "index_file_path = \"inverted_index.json\"\n",
    "with open(index_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "    json.dump(inverted_index, json_file, ensure_ascii=False, indent=4)\n",
    "print(f\"Το ανεστραμμένο ευρετήριο αποθηκεύτηκε στο {index_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d88e8462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης:\n",
      "Title: Cancer - Score: 1.40\n"
     ]
    }
   ],
   "source": [
    "# Αναζήτηση και εμφάνιση αποτελεσμάτων\n",
    "query = \"Doctor\"\n",
    "results = process_query_bm25(query, all_data, inverted_index)\n",
    "\n",
    "print(\"Αποτελέσματα αναζήτησης:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Title: {all_data[doc_id]['title']} - Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ffce9846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης:\n",
      "Title: Diabetes\n",
      "Title: Data mining\n",
      "Title: Artificial intelligence\n",
      "Title: University of West Attica\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Αναζήτηση και εμφάνιση αποτελεσμάτων\n",
    "query = \"University\"  # Παράδειγμα ερώτημα Boolean\n",
    "results = process_query_boolean(query, inverted_index)\n",
    "\n",
    "print(\"Αποτελέσματα αναζήτησης:\")\n",
    "for doc_id in results:\n",
    "    print(f\"Title: {all_data[doc_id]['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9fdcc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης:\n",
      "Title: Artificial intelligence - Score: 8.79\n",
      "Title: Chocolate - Score: 2.20\n",
      "Title: McDonald's - Score: 1.10\n",
      "Title: Gastroenteritis - Score: 1.10\n"
     ]
    }
   ],
   "source": [
    "# Αναζήτηση και εμφάνιση αποτελεσμάτων\n",
    "query = \"Plant\"  # Παράδειγμα ερώτημα για TF-IDF\n",
    "query_tokens = word_tokenize(preprocess_text(query).lower())  # Επεξεργασία ερωτήματος\n",
    "results = calculate_tfidf(query_tokens, all_data, inverted_index)\n",
    "\n",
    "print(\"Αποτελέσματα αναζήτησης:\")\n",
    "for doc_id, score in results:\n",
    "    print(f\"Title: {all_data[doc_id]['title']} - Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e440d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Αποτελέσματα αναζήτησης (VSM):\n",
      "Title: Data mining - Score: 0.0756\n",
      "Title: Web scraping - Score: 0.0115\n",
      "Title: Artificial intelligence - Score: 0.0036\n",
      "Title: Natural language processing - Score: 0.0022\n",
      "Title: Diabetes - Score: 0.0004\n",
      "Title: Cancer - Score: 0.0003\n"
     ]
    }
   ],
   "source": [
    "# Εκτέλεση αναζήτησης με VSM\n",
    "query = \"Data mining\"\n",
    "results_vsm = process_query_vsm(query, all_data, inverted_index)\n",
    "\n",
    "print(\"Αποτελέσματα αναζήτησης (VSM):\")\n",
    "for doc_id, score in results_vsm:\n",
    "    print(f\"Title: {all_data[doc_id]['title']} - Score: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
